{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Introduction to LangGraph & State Management\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what LangGraph is and why we need it\n",
    "- Learn core concepts: nodes, edges, state, checkpointers\n",
    "- Build a simple stateful chatbot with conversation memory\n",
    "- Compare LangGraph with LangChain chains\n",
    "\n",
    "**Prerequisites:** RAG with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What is LangGraph?\n",
    "\n",
    "### The Problem with Chains\n",
    "\n",
    "You have learned about **LangChain chains** - they're great for fixed pipelines:\n",
    "\n",
    "```python\n",
    "# Always follows this path:\n",
    "chain = retriever | prompt | llm | parser\n",
    "```\n",
    "\n",
    "But what if you need:\n",
    "- **Decisions during execution?** (\"Should I retrieve or not?\")\n",
    "- **Loops and cycles?** (\"Try again if answer is poor\")\n",
    "- **Multiple tools?** (\"Use search OR calculator OR retrieval\")\n",
    "- **Complex control flow?** (\"If X then Y, else Z\")\n",
    "\n",
    "**That's where LangGraph comes in!**\n",
    "\n",
    "### LangGraph = State Machines for Agents\n",
    "\n",
    "LangGraph lets you build **graphs** where:\n",
    "- **Nodes** are functions that process state\n",
    "- **Edges** connect nodes (fixed or conditional)\n",
    "- **State** flows through the graph\n",
    "- **Agents make decisions** about which path to take\n",
    "\n",
    "```\n",
    "           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  START ‚îÄ‚îÄ‚ñ∂‚îÇ  Node 1  ‚îÇ\n",
    "           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "                ‚ñº\n",
    "           Decision?\n",
    "           ‚îú‚îÄ YES ‚îÄ‚ñ∂ Node 2 ‚îÄ‚îÄ‚îê\n",
    "           ‚îî‚îÄ NO  ‚îÄ‚ñ∂ Node 3 ‚îÄ‚îÄ‚î§\n",
    "                               ‚ñº\n",
    "                              END\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangGraph Application Architecture](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/application.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install -q langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up OpenAI API Key\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"gen_api_key\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM\n",
    "\n",
    "We'll use **GPT-4o-mini** - it's fast and cost-effective for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Core Concept #1 - State\n",
    "\n",
    "**State** ‚ÄúThe single source of truth for the whole agent execution.‚Äù  \n",
    "**State** is the data that flows through your graph.\n",
    "\n",
    "### MessagesState\n",
    "\n",
    "For chatbots, LangGraph provides `MessagesState` - it stores conversation history:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        AIMessage(content=\"Hi! How can I help?\"),\n",
    "        HumanMessage(content=\"What's Python?\"),\n",
    "        # ... more messages\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is similar to `ConversationBufferMemory`, but more flexible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Reflection Question:** \n",
    "How is this different from ConversationBufferMemory? In chains, memory was managed separately. In LangGraph, it's part of the state that flows through nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Concept #2 - Nodes\n",
    "\n",
    "**Nodes** are functions that process state and return updates.\n",
    "\n",
    "### Node Function Signature\n",
    "\n",
    "```python\n",
    "def my_node(state: MessagesState) -> dict:\n",
    "    # Process state\n",
    "    # Return updates to state\n",
    "    return {\"messages\": [new_message]}\n",
    "```\n",
    "\n",
    "### The Assistant Node\n",
    "\n",
    "Let's create our first node - it sends messages to the LLM and gets a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Assistant node defined\n"
     ]
    }
   ],
   "source": [
    "# System prompt that defines assistant behavior\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a friendly assistant that answers user questions. Be helpful and concise.\"\n",
    ")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    The assistant node - processes messages and generates response.\n",
    "    \"\"\"\n",
    "    # Combine system prompt with conversation history\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return as state update\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "print(\"‚úÖ Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Point:** The node doesn't modify state directly - it returns updates that LangGraph applies automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Retriever Node\n",
    "You don't need to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_docs(state: MessagesState):\n",
    "#     query = state[\"messages\"][-1].content  # latest HumanMessage\n",
    "#     docs = retriever.invoke(query)\n",
    "\n",
    "#     return {\n",
    "#         \"messages\": [\n",
    "#             ToolMessage(\n",
    "#                 content=\"\\n\".join(d.page_content for d in docs),\n",
    "#                 name=\"retriever\"\n",
    "#             )\n",
    "#         ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Concept 2B - Edges\n",
    "\n",
    "**Edges** are the connections between nodes that control the flow of your agent.\n",
    "\n",
    "Think of edges as **roads** between cities (nodes). They determine which node to visit next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Edges\n",
    "\n",
    "LangGraph has two types of edges:\n",
    "\n",
    "1. **Fixed/Static Edges** (Normal Edges)\n",
    "   - Always go from Node A to Node B\n",
    "   - No decision-making\n",
    "   - Used for predictable flows\n",
    "\n",
    "2. **Conditional Edges**\n",
    "   - Decide which node to visit next based on state\n",
    "   - Enable agent decision-making\n",
    "   - Used for dynamic, intelligent behavior\n",
    "\n",
    "```\n",
    "Fixed/Static Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Node B  (always goes to B)\n",
    "\n",
    "Conditional Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Decision?\n",
    "                    ‚îú‚îÄ Condition 1 ‚îÄ‚îÄ‚ñ∂ Node B\n",
    "                    ‚îú‚îÄ Condition 2 ‚îÄ‚îÄ‚ñ∂ Node C\n",
    "                    ‚îî‚îÄ Condition 3 ‚îÄ‚îÄ‚ñ∂ Node D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Core Concept #3 - Building the Graph\n",
    "\n",
    "Now let's connect everything into a **StateGraph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph structure defined\n"
     ]
    }
   ],
   "source": [
    "# Create a StateGraph with MessagesState\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add the assistant node\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "# Define the flow:\n",
    "# START ‚Üí assistant ‚Üí END\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "print(\"‚úÖ Graph structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Flow\n",
    "\n",
    "```\n",
    "START ‚Üí [assistant node] ‚Üí END\n",
    "```\n",
    "\n",
    "- **START:** Entry point (receives user message)\n",
    "- **assistant:** Processes message and generates response\n",
    "- **END:** Exit point (returns final state)\n",
    "\n",
    "This is simple now, but later we'll add conditional edges for agentic behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Core Concept #4 - Checkpointers (Memory)\n",
    "\n",
    "**Checkpointers** save state between interactions - this gives our agent memory!\n",
    "\n",
    "Without checkpointer:\n",
    "- Each call starts fresh\n",
    "- No conversation history\n",
    "- Agent forgets everything\n",
    "\n",
    "With checkpointer:\n",
    "- State persists between calls\n",
    "- Agent remembers conversation\n",
    "- Multi-turn conversations work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent compiled with memory\n"
     ]
    }
   ],
   "source": [
    "# Create a memory checkpointer (stores in memory)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph WITH memory\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agent compiled with memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîß Production Note:** `MemorySaver` stores in RAM (lost on restart). For production, use:\n",
    "- `SqliteSaver` - persists to SQLite database\n",
    "- `MongoDBSaver` - persists to MongoDB\n",
    "\n",
    "We'll use MemorySaver for learning since it's simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Visualizing the Graph\n",
    "\n",
    "One of LangGraph's best features - **visual representation** of your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START ‚Üí assistant ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Pro Tip:** Always visualize your graph! It helps you understand and debug agent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Running the Agent\n",
    "\n",
    "Now let's actually use our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session IDs (Thread IDs)\n",
    "\n",
    "Each conversation has a unique **thread_id**. Messages with the same thread_id share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with session ID: chat-session-0012\n"
     ]
    }
   ],
   "source": [
    "# Define a session ID for this conversation\n",
    "session_id = \"chat-session-0012\"\n",
    "\n",
    "print(f\"Starting conversation with session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversation function ready\n"
     ]
    }
   ],
   "source": [
    "def run_conversation(user_input: str, thread_id: str = session_id):\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get response.\n",
    "    ‚ö†Ô∏è WARNING: Using default thread_id shares conversation acrosss all calls!\n",
    "    In production, ALWAYS provide unique thread_id per user.\n",
    "    \"\"\"\n",
    "    # Invoke the agent\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Print the conversation\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nüë§ User: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"ü§ñ Agent: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Conversation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Single Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"Hello! What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Multi-Turn Conversation (Memory Test!)\n",
    "\n",
    "Now let's test if the agent remembers context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's a lovely choice! Blue is often associated with calmness, serenity, and stability. Do you have a favorite shade of blue?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "run_conversation(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's a lovely choice! Blue is often associated with calmness, serenity, and stability. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! If you have any questions or want to talk more about colors, feel free to ask!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question - does it remember?\n",
    "run_conversation(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: I'm not sure what your favorite color is! If you'd like to share it, I‚Äôd love to hear.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"What's my favorite color?\", thread_id=\"111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Success!** The agent remembered your favorite color because:\n",
    "1. The checkpointer saved the state after the first message\n",
    "2. The same thread_id retrieved that saved state\n",
    "3. The conversation history was passed to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Context-Dependent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's a lovely choice! Blue is often associated with calmness, serenity, and stability. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! If you have any questions or want to talk more about colors, feel free to ask!\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's great! RAG stands for Retrieval-Augmented Generation. It's a method that combines retrieval of relevant documents or information with generative models to enhance the quality of responses. This approach is particularly useful in scenarios where generating accurate and contextually relevant answers is important.\n",
      "\n",
      "If you have specific questions about RAG systems or want to know more about a particular aspect, feel free to ask!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Start a new topic\n",
    "run_conversation(\"I'm learning about RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's a lovely choice! Blue is often associated with calmness, serenity, and stability. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! If you have any questions or want to talk more about colors, feel free to ask!\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's great! RAG stands for Retrieval-Augmented Generation. It's a method that combines retrieval of relevant documents or information with generative models to enhance the quality of responses. This approach is particularly useful in scenarios where generating accurate and contextually relevant answers is important.\n",
      "\n",
      "If you have specific questions about RAG systems or want to know more about a particular aspect, feel free to ask!\n",
      "\n",
      "üë§ User: Can you explain the main components?\n",
      "ü§ñ Agent: Certainly! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: This part is responsible for fetching relevant documents or information from a large corpus or database. It often uses techniques like vector-based retrieval or traditional keyword-based search to identify the most pertinent sources based on the input query.\n",
      "\n",
      "2. **Generative Model**: This is usually a transformer-based model (like GPT) that generates text. After retrieving relevant documents, the generative model takes this information and uses it to create coherent and contextually appropriate responses.\n",
      "\n",
      "3. **Combiner**: This component integrates the retrieved information with the generative model‚Äôs output. It ensures that the generated response is not only coherent but also relevant to the retrieved documents, often by conditioning the generation on the retrieved content.\n",
      "\n",
      "4. **Training Data**: RAG systems require a dataset that includes both queries and corresponding documents, as well as the desired outputs. This data is used to train the retrieval and generation components effectively.\n",
      "\n",
      "5. **Evaluation Metrics**: To assess performance, RAG systems often use metrics that evaluate both retrieval quality (e.g., precision, recall) and generation quality (e.g., BLEU, ROUGE).\n",
      "\n",
      "These components work together to enable the system to provide more accurate and context-aware responses than a typical generative model alone. If you have more questions about any of these components, let me know!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reference it\n",
    "run_conversation(\"Can you explain the main components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's a lovely choice! Blue is often associated with calmness, serenity, and stability. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! If you have any questions or want to talk more about colors, feel free to ask!\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's great! RAG stands for Retrieval-Augmented Generation. It's a method that combines retrieval of relevant documents or information with generative models to enhance the quality of responses. This approach is particularly useful in scenarios where generating accurate and contextually relevant answers is important.\n",
      "\n",
      "If you have specific questions about RAG systems or want to know more about a particular aspect, feel free to ask!\n",
      "\n",
      "üë§ User: Can you explain the main components?\n",
      "ü§ñ Agent: Certainly! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: This part is responsible for fetching relevant documents or information from a large corpus or database. It often uses techniques like vector-based retrieval or traditional keyword-based search to identify the most pertinent sources based on the input query.\n",
      "\n",
      "2. **Generative Model**: This is usually a transformer-based model (like GPT) that generates text. After retrieving relevant documents, the generative model takes this information and uses it to create coherent and contextually appropriate responses.\n",
      "\n",
      "3. **Combiner**: This component integrates the retrieved information with the generative model‚Äôs output. It ensures that the generated response is not only coherent but also relevant to the retrieved documents, often by conditioning the generation on the retrieved content.\n",
      "\n",
      "4. **Training Data**: RAG systems require a dataset that includes both queries and corresponding documents, as well as the desired outputs. This data is used to train the retrieval and generation components effectively.\n",
      "\n",
      "5. **Evaluation Metrics**: To assess performance, RAG systems often use metrics that evaluate both retrieval quality (e.g., precision, recall) and generation quality (e.g., BLEU, ROUGE).\n",
      "\n",
      "These components work together to enable the system to provide more accurate and context-aware responses than a typical generative model alone. If you have more questions about any of these components, let me know!\n",
      "\n",
      "üë§ User: Which component is most important?\n",
      "ü§ñ Agent: The importance of each component in a Retrieval-Augmented Generation (RAG) system can vary depending on the specific use case and context. However, generally speaking:\n",
      "\n",
      "1. **Retrieval Component**: This is often considered the most critical component because the quality and relevance of the retrieved documents significantly influence the generated output. If the retrieval is poor, the generative model may not have the necessary context to produce accurate or meaningful responses.\n",
      "\n",
      "2. **Generative Model**: While the retrieval component sets the foundation, the generative model is crucial for producing coherent and contextually appropriate text. Its ability to synthesize information from the retrieved documents determines the final response quality.\n",
      "\n",
      "3. **Combiner**: This component is essential for integrating the retrieved information with the generative model's output effectively. A well-designed combiner can enhance the overall coherence and relevance of the response.\n",
      "\n",
      "In summary, while all components are important, the retrieval component is often viewed as the backbone of a RAG system, as it directly impacts the effectiveness of the generative process. Ultimately, the best performance comes from a well-balanced and optimized integration of all components.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "run_conversation(\"Which component is most important?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Notice:** The agent maintains context across multiple turns - just like a real conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Multiple Conversations (Different Thread IDs)\n",
    "\n",
    "Let's test that different thread IDs have separate memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ CONVERSATION 1\n",
      "\n",
      "üë§ User: My name is Alice\n",
      "ü§ñ Agent: Nice to meet you, Alice! How can I assist you today?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üü¢ CONVERSATION 2\n",
      "\n",
      "üë§ User: My name is Bob\n",
      "ü§ñ Agent: Nice to meet you, Bob! How can I assist you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Conversation 1\n",
    "print(\"\\nüîµ CONVERSATION 1\")\n",
    "run_conversation(\"My name is Alice\", thread_id=\"user_alicee\")\n",
    "\n",
    "# Conversation 2 (different user)\n",
    "print(\"\\nüü¢ CONVERSATION 2\")\n",
    "run_conversation(\"My name is Bob\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ BACK TO CONVERSATION 1\n",
      "\n",
      "üë§ User: My name is Alice\n",
      "ü§ñ Agent: Nice to meet you, Alice! How can I assist you today?\n",
      "\n",
      "üë§ User: What's my name?\n",
      "ü§ñ Agent: Your name is Alice! How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Alice - does it remember her name?\n",
    "print(\"\\nüîµ BACK TO CONVERSATION 1\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_alicee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ BACK TO CONVERSATION 2\n",
      "\n",
      "üë§ User: My name is Bob\n",
      "ü§ñ Agent: Nice to meet you, Bob! How can I assist you today?\n",
      "\n",
      "üë§ User: What's my name?\n",
      "ü§ñ Agent: Your name is Bob! How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Bob\n",
    "print(\"\\nüü¢ BACK TO CONVERSATION 2\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Key Insight:** Each thread_id maintains its own conversation history. This is how you'd handle multiple users in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ü§ñ Interactive Chat Started\n",
      "Type your message and press Enter. Type 'exit' to quit.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ü§ñ Agent: Hello! How can I assist you today?\n",
      "\n",
      "üëã Goodbye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session.\n",
    "    Type 'exit' or 'quit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§ñ Interactive Chat Started\")\n",
    "    print(\"Type your message and press Enter. Type 'exit' to quit.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    thread_id = \"interactive_session2\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nüëã Goodbye!\\n\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "        \n",
    "        # Print agent's response\n",
    "        agent_message = result[\"messages\"][-1]\n",
    "        print(f\"\\nü§ñ Agent: {agent_message.content}\")\n",
    "\n",
    "# Uncomment to run interactive chat:\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: ConversationalRetrievalChain\n",
    "\n",
    "Let's compare LangGraph with the memory you learned in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "\n",
    "```python\n",
    "# Langchain approach\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is Python?\"})\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Simple to use\n",
    "- ‚úÖ Built-in memory\n",
    "- ‚ùå Fixed pipeline (always retrieves)\n",
    "- ‚ùå No conditional logic\n",
    "- ‚ùå Can't add complex decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12: LangGraph Agent\n",
    "\n",
    "```python\n",
    "# LangGraph approach\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant_node)\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is Python?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Flexible - add any nodes/edges\n",
    "- ‚úÖ Conditional logic (coming in Topic 2)\n",
    "- ‚úÖ Agents can make decisions\n",
    "- ‚úÖ Supports cycles and loops\n",
    "- ‚ùå More complex to set up\n",
    "- ‚ùå Requires understanding graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What?\n",
    "\n",
    "| Use Case | LangChains | LangGraph |\n",
    "|----------|-------------------|------------------------|\n",
    "| Simple chatbot | ‚úÖ Perfect | ‚ö†Ô∏è Overkill |\n",
    "| Fixed RAG pipeline | ‚úÖ Great | ‚ö†Ô∏è Unnecessary |\n",
    "| Agent with tools | ‚ùå Limited | ‚úÖ Ideal |\n",
    "| Conditional retrieval | ‚ùå Can't do | ‚úÖ Perfect |\n",
    "| Multi-agent systems | ‚ùå Not possible | ‚úÖ Built for it |\n",
    "\n",
    "**Rule of thumb:** If you need decision-making during execution, use LangGraph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **LangGraph Basics**\n",
    "   - LangGraph enables agentic behavior through graphs\n",
    "   - Better than chains when you need decisions during execution\n",
    "\n",
    "2. **Core Concepts**\n",
    "   - **State:** Data flowing through the graph (MessagesState for chat)\n",
    "   - **Nodes:** Functions that process and update state\n",
    "   - **Edges:** Connections between nodes (fixed or conditional)\n",
    "   - **Checkpointers:** Persist state for memory across sessions\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Built a stateful chatbot\n",
    "   - Used thread_id for multi-user conversations\n",
    "   - Visualized graph structure\n",
    "   - Compared with Module 9 chains\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Topic 2: Tool Integration**\n",
    "- Add tools for agents (search, calculator, retrieval)\n",
    "- Conditional edges (agent decides which tool to use)\n",
    "- This is where LangGraph really shines!\n",
    "\n",
    "**Topic 3: Agentic RAG**\n",
    "- Agent that decides when to retrieve\n",
    "- Combines everything from Topics 1-2\n",
    "- The core concept of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "## Exercise 1: Build Your First Stateful Agent\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "**Estimated Time:** 30-45 minutes\n",
    "\n",
    "### Task\n",
    "Build a simple customer support chatbot that remembers conversation context.\n",
    "\n",
    "### Requirements\n",
    "1. Create a StateGraph with MessagesState\n",
    "2. Add a system prompt that makes the agent act as a helpful customer support rep\n",
    "3. Use MemorySaver checkpointer for memory\n",
    "4. Test with a multi-turn conversation where context matters\n",
    "\n",
    "### Example Conversation\n",
    "```\n",
    "User: \"I bought a laptop last week\"\n",
    "Agent: \"I'd be happy to help with your laptop! What seems to be the issue?\"\n",
    "User: \"It won't turn on\"\n",
    "Agent: \"I understand your laptop won't turn on. Have you tried...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does LangGraph's state management differ from ConversationalRetrievalChain memory?**\n",
    "   \n",
    "2. **Why do we need thread_id for conversations?**\n",
    "   \n",
    "3. **What happens if you don't configure a checkpointer?**\n",
    "   \n",
    "4. **When would you choose chains over LangGraph?**\n",
    "   \n",
    "5. **How would you debug an agent that's not behaving correctly?**\n",
    "\n",
    "Write your answers below or discuss with your study group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üéâ Topic 1 Complete!** \n",
    "\n",
    "You now understand LangGraph fundamentals. Next up: **Tool Integration** - where agents become truly powerful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
